{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data jobs analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              column_name\n",
      "0                  job_id\n",
      "1              company_id\n",
      "2      job_work_from_home\n",
      "3         job_posted_date\n",
      "4   job_no_degree_mention\n",
      "5    job_health_insurance\n",
      "6         salary_year_avg\n",
      "7         salary_hour_avg\n",
      "8         search_location\n",
      "9             salary_rate\n",
      "10        job_title_short\n",
      "11              job_title\n",
      "12           job_location\n",
      "13                job_via\n",
      "14      job_schedule_type\n",
      "15            job_country\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Establish a database connection\n",
    "engine = create_engine('postgresql://postgres:191405@localhost:5432/job_analyse')\n",
    "\n",
    "# Write the SQL query\n",
    "query = '''\n",
    "SELECT column_name\n",
    "FROM information_schema.columns\n",
    "WHERE table_name = 'job_postings_fact';\n",
    "'''\n",
    "\n",
    "# Execute the query and load the result into a DataFrame\n",
    "df = pd.read_sql_query(query, engine)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Inspect the DataFrame to check column names\n",
    "print(df.head(16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting humanize\n",
      "  Downloading humanize-4.11.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Downloading humanize-4.11.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: humanize\n",
      "Successfully installed humanize-4.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install humanize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:/Windows/Temp/gsearch_jobs.csv')\n",
    "\n",
    "# Function to convert \"X hours ago\" format to datetime\n",
    "def convert_to_timestamp(value):\n",
    "    if isinstance(value, str) and 'hours ago' in value:\n",
    "        hours_ago = int(value.split()[0])\n",
    "        return (datetime.now() - timedelta(hours=hours_ago)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return value\n",
    "\n",
    "# Apply the conversion to the 'posted_at' column\n",
    "df['posted_at'] = df['posted_at'].apply(convert_to_timestamp)\n",
    "\n",
    "# Save the modified DataFrame back to CSV\n",
    "df.to_csv('C:/Windows/Temp/gsearch_jobs_modified.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the original file in a more flexible encoding and write it to a new file with UTF-8 encoding\n",
    "with open('C:/Windows/Temp/gsearch_jobs_modified.csv', 'r', encoding='utf-8-sig') as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "# Write the content to a new file with UTF-8 encoding\n",
    "with open('C:/Windows/Temp/gsearch_jobs_utf8.csv', 'w', encoding='utf-8') as outfile:\n",
    "    outfile.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed and file saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the cleaned file with ISO-8859-1 encoding\n",
    "df = pd.read_csv('C:/Windows/Temp/jobs_analyze.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Drop duplicate rows based on 'job_id'\n",
    "df = df.drop_duplicates(subset='job_id')\n",
    "\n",
    "# Save the cleaned file again\n",
    "df.to_csv('C:/Windows/Temp/jobs_analyze_no_duplicates.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Duplicates removed and file saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
